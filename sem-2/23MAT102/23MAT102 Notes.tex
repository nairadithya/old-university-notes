\documentclass{report}

\input{preamble}
\title{\Huge{23MAT102}\\ Class Notes}
\author{\huge{Adithya Nair}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\tableofcontents

\pagebreak
\section{A Basic Order Of Importance}
\begin{itemize}
    \item \textbf{Axiom} - Statements taken as fact
    \item \textbf{Theorem} - Statements that are proven using axioms
    \item \textbf{Lemma} - Statements proven using theorems
    \item \textbf{Proposition} - Statements, regardless of whether it is true or false, is assumed to be true
    \item \textbf{Corollary} - A theorem that is proven using another theorem.*
\end{itemize}
\chapter{A Revision Of Sets And Functions}
Sets are assumed to be sets on the basis of a theory known as \textbf{Naive Set Theory}. according to this theory, A set is defined as,

\begin{definition}[Sets]
A set is a collection of objects
\end{definition}

e.g. - \[\mathbb{N}, \mathbb{Z}, \mathbb{Q}, \mathbb{R}, \mathbb{C}\]

\section{Notations}
A,B...Z will denote sets
a,b...z will denote elements
a $\in$ A, a is an element of A
a $\notin$  A, a is not an element of A

\section{Roster Notation}
\[\mathbb{N} = \{1,2...\}\]
\[A = \{2,4,6,8\]
\[B = \{ x \in Z+| x < 10 \} \]
 B is written in set builder form

\section{Basic Concepts Of Sets}
\begin{definition}[Subsets]
A and B are two sets. A is a subset of B, and we write A $\subset$ B, if every element of A is also an element of B
\end{definition}

\begin{theorem}
    Two sets A and B are equal and we write A=B if and only if A $\subset$ B and B $\subset$ A
\end{theorem}

\begin{definition}[Unions]
    The union of two sets A and B, denoted by A $\cup$ B, is
    \[ A \cup B = \{x|\ x \in A \ and \ x \in B\}\]
\end{definition}
\begin{definition}[Intersections]
    The intersection of two sets denoted by A $\cap$ B, is
    \[A \cap B = \{x|\ x \in A \ or \ x \in B\}\]
\end{definition}
\begin{definition}[Set Difference]
    The difference of two sets denoted by A$\setminus$ B is
    \[A\setminus B = {x|\ x \in A \ and \ x \notin B}\]
\end{definition}

\begin{definition}[Set Complement]
    The complement of a set A, denoted by A $^{C}$ is,
    \[A^{C} = \{x \in X |\ x \notin A \}\]
\end{definition}
\begin{itemize}
    \item $(B \cup C)^{C} = B^{C}  \cap C^{C}$
    \item $(B \cap C)^{C} = B^{C} \cap C^{C}$
    \item $A\setminus (B\cup C) = (A\setminus B) \cap (A \setminus C)$
    \item $A\setminus (B\cap C) = (A\setminus B) \cup (A \setminus C)$
\end{itemize}
\section{Logical Notation}
$\forall$ - for all
$\exists$ - there exists
$\exists!$ - there exists a unique

\section{Functions}
f: A $\rightarrow$ B
f(a) = b, a $\in$ A, b $\in$ B
A is the \textbf{domain} of the function, B is the \textbf{codomain} of the function and, 
\{b$\in$ B \textbar f(a) = b \} - Range
\section{Cartesian Product}
\[A \times B = \{(a,b) | a \in A, b \in B\}\]



\section{Composition Of Functions}
(g$\circ$f)(x) = (g(f(x))

A function is the same as a mapping, which is the same as a transformation 
\section{Types Of Functions}
\begin{enumerate}
    \item f is injective(one-one) if,
    \[f(a) = f(a')\ then\ a\ =\ a'\]
    \item f is surjective(onto) if, 
    \[\forall b \in B, \exists \ a \in A, \  f(a) = b\]
    \item f is bijective if f is injective and surjective
    \end{enumerate}   
\section*{Reference}
Knowles - Linear Vector Spaces and Cartesian Tensors\\
Halmos - Finite Dimensional Linear Spaces\\
Gelfand - Linear Algebra
\chapter{Linear Algebra}
A vector space over a field F = $\mathbb{R}$ or $\mathbb{C}$ is a set V with two operations:
\begin{enumerate}
\item +:V$\times$ V $\rightarrow$ V i.e. "+" is closed under addition.

\item $\cdot$:F $\times$ V $\rightarrow$ V, i.e. $"\cdot"$ is closed under multiplication
\end{enumerate}
having the following properties

\begin{enumerate}
    \item \textbf{Associativity}
    \[\forall \ v_1, v_2,v_3 \in V, (v_1 + v_2) + v_3 = v_1 + (v_2 + v_3)\]
    \item \textbf{Existence of identity element}
    \[\exists! \ 0 \in V, \forall v\ in V, such that 0 + v = v\]
    \item \textbf{Existence of additive inverse}
    \[\forall \ v \in V \exists (-v) \in V, v + (-v) = 0\]
    \item \textbf{Commutativity}
    \[\forall \ u, v \in V, u+v = v+u \]
        Properties 1 to 4 constitute a group known as the "abelian group" or "commutative group"
    \item \textbf{Existence of multiplactive identity}
    \[\exists! \ 1 \in V, \ such \ that \ \forall \ v \in V, 1\cdot v = v\]
    \item \textbf{Associativity}
    \[\mu, \lambda \in F, v \in V, \lambda(\mu \cdot v) = (\lambda\mu)\cdot v\]
    \item \textbf{Distribution of + over $\cdot$}
    \[(\lambda + \mu) \cdot v = \lambda \cdot v + \mu \cdot v, \forall \ \mu, \lambda \in F\]
    \item \textbf{Distribution of $\cdot$ over +}
    \[\lambda \cdot(u + v)\ = \lambda \cdot u + \lambda \cdot v, \forall \lambda \in \ F, u,v \in V\]    
\end{enumerate}
\section{Examples Of Vector Spaces}
\begin{enumerate}
    \item V = {0}
    \item $\mathbb{R}$
    \item All polynomials of order \textbf{at most} n
\end{enumerate}
\section*{Reference}
    \begin{itemize}
        \item Donald Knuth
        \item Marvin Mirsky, MIT
        \item Web Of Stories, Youtube Channel
        \item Axler, Chapter 1
        \item Olver, Shakiban, Chapter 2
        \item Terrence Tao Notes - AMS Open Math
    \end{itemize}
    \section{Some Theorems And Proofs Regarding Vector Spaces}
\begin{theorem}
    Additive identity is unique
\end{theorem}
    \begin{proof}
        Suppose $\exists \ additive \ identities \ 0_1$, $0_2$ such that
        \[\forall u \in V, 0_1 + u \ \& \ 0_2 + u = u\]
        \[0_1 + 0_2 = 0_2\]
        \[0_2 + 0_1 = 0_1\]
        \[\therefore 0_1 = 0_2\]
    \end{proof}
\begin{theorem}
    Additive inverse is unique
\end{theorem}
    \begin{proof}
        Suppose additive inverses of u are $v_1, v_2$
        \[u + v_1 = 0, u + v_2 = 0\]
        \[v_2 + (u+v_1) = v_2 + 0\]
        \[(v_2 + u) + v_1 = v_2\]
        \[0 + v_1 = v_2\]
        \[v_1 = v_2\]
        \end{proof}
\begin{theorem}
    $0\cdot u = 0$
\end{theorem}
\begin{proof}

   Let 0 . u = 0 
   Consider,
   \[
   v + v = 0.u + 0.u = (0+0).u
   \]
\[
	= 0.u = v
\]	
\[
	\implies v+v = v \\
\]
\[
       v+(v + (-v)) = v + -v \\
\]
\[
	\implies v = 0 
\]
\end{proof}
\begin{theorem}[Scalars And Inverses]
	\[
	   (-\lambda)u = -(\lambda.u) = \lambda . (-u)
	\]
\end{theorem}
\begin{proof}
	Let \[
	   v = (-\lambda).u
	\]
	Consider,
	\begin{displaymath}
	v + \lambda. u = (-\lambda).u + \lambda. u
	\end{displaymath}
        \[
            \indent = (\lambda + \lambda).u
        \]
        \[
            = 0.u = 0
        \]	
        \[
            \therefore (\lambda .u) + -(\lambda.u) = 0
        \]
        \[
            =(-\lambda.u) + (\lambda.u) + -(\lambda.u) = (-\lambda.u)
        \]
        \[
            = (-\lambda).u + 0 = (-\lambda.u)  
        \]
        \[
            (-\lambda).u = -(\lambda.u)
        \]
\end{proof}

\section{Fields}
A) To every pair $\alpha$ and $\beta$ of scalars, there corresponds a scalar $\alpha + \beta$ called the sum of $\alpha \text{ and } \beta$, in such a way that 
\begin{enumerate}
	\item addition is commutative, $\alpha + \beta = \beta + \alpha$
	\item addition is associative, $(\alpha + \beta)+\gamma = \alpha + (\beta + \gamma)$   
	\item There exists a unique scalar 0, called zero, such that $\alpha + 0 = \alpha$ for every scalar $\alpha$, and 
	\item to every scalar $\alpha$, there corresponds a unique scalar $(-\alpha)$ such that $\alpha + (-\alpha) = 0$ 
\end{enumerate}
B) To every pair $\alpha$ and $\beta$ of scalars there corresponds a scalar $\alpha\beta$, called the product of $\alpha$ and $\beta$ in such a way that
\begin{enumerate}
    \item multiplication is commutative, $\alpha\beta = \beta\alpha$
    \item multiplication is associative, $(\alpha\beta)\gamma = \alpha(\beta\gamma)$
    \item there exists a unique non-zero scalar 1(called one) such that $\alpha 1 = \alpha$ for every scalar $\alpha$ and
    \item to every non-zero scalar $\alpha$, there corresponds a unique scalar $\alpha^{-1} = \frac{1}{\alpha}$ such that $\alpha \alpha^{-1} = 1$
\end{enumerate}
C) Multiplication is distributive with respect to addition,
\[
    \alpha(\beta + \gamma) = (\alpha\beta + \alpha \gamma )
\]
If addition and multiplication are defined within same set of objects(scalars) so that the conditions A,B and C are satisfied, then that set is called a field.

\begin{note}
    The main difference between vector spaces and fields: \\
    All fields are vector spaces over themselves. 
    The main difference arises in the operation. 
    Vector spaces have operations:
    \[
        +: V \times V \rightarrow V
    \]
    \[
        \cdot : F \times V \rightarrow V
    \]
    While fields have both operations:
    \[
        +, \cdot : F \times F \rightarrow F
    \]
    Another key difference is that fields have an axiom regarding a unique scalar known as the multiplicative inverse $(\alpha^{-1})$ which is not an axiom for vector spaces.
\end{note}
\section{Examples Of Vector Spaces And Fields}
Examples of fields include: $\mathbb{Q},\mathbb{R},\mathbb{C}$
\begin{itemize}
    \item For complex numbers \[\mathbb{C}\text{(Complex Numbers)} = \{(a,b): a,b \in R\}\]
        \[
            (a,b)+(c,d) = (a+c,b+d) \\
        \]
        \[
            (a,b)(c,d) = (ac-bd,ad+bc)
        \]
        This is a field.
    \item $\mathbb{R}^n$, n-tuple where $x = (x_1,x_2,x_2 \dots x_n)$
        This is a vector space over $\mathbb{R}$
    \item $P_n(\mathbb{R})$, the set of all polynomials with degree n is a vector space over $\mathbb{R}$.
    \begin{note}
        $P_n(\mathbb{R})$ has a direct correlation to $(\mathbb{R}^{n+1})$ which means they are \textbf{isomorphic} spaces
    \end{note}
    \item C($\mathbb{R}$) - the space of all continuous functions is a vector space over field $\mathbb{R}$
    \item $M_{m \times n}$ : set of all $m \times n$ matrices is a vector space
    \item \textbf{Linear maps/operations/transformations/functions} - 
        Suppose U,V are two vector spaces over a field F, then $T:u \rightarrow v$ is linear for some scalars $\alpha,\beta \in F$, we have $T(\alpha u_1 + \beta u_2 = \alpha T(u_1)) + \beta T(u_2)$
\end{itemize}

\section{Subspaces}
(V,+, $\cdot$) - is a vector space \\
(W $\subset$ V, +, $\cdot$) - is a subspace of V

A subspace is a vector space, where the set is a subset of another vector space.

\begin{note}
    All lines that pass through the origin in a 2-dimensional plane are subspaces of $\mathbb{R}^2$
\end{note}
\pagebreak
\begin{lemma}
    Let V be a vector space and let W be a subset of V . Then W is a subspace of V if and only if the following properties hold:
    \begin{enumerate}
        \item W is closed under addition, 
            \begin{align*}
                \text{If } w_1, w_2 \in W, \text{ then} \\
            w_1 + w_2 \in W
            \end{align*}
        \item W is closed under scalar multiplication.
            \[
                \text{If }  \alpha \in F, w \in W, \text{ then } \alpha w \in W
            \]
    \end{enumerate}
\end{lemma}
\begin{proof}
    \begin{enumerate}
        \item W is a subspace $\implies$ 1. and 2.\\
            W is closed under addition and scalar multiplication, because this is implicit in the definition of subspaces.
        \item 1. and 2. $\implies$ W is a subspace  \\
            $-u \in W$, because $-1.u = -u$ \\
            $0 \in W$ because $0.u = 0$
    \end{enumerate}
\end{proof}
\section{Span}
\begin{definition}[Spans]
    Let S be a finite collection of vectors in a vector space V. 
    A linaer combination of S is defined to be any vector in V of the form 
    \[
        \alpha_1 v_1 + \alpha_2 v_2 \dots + \alpha_n v_n
    \]
\end{definition}
\begin{theorem}
    Let S be a subset of a vector space V. Then span(S) is a subspace of V which contains S is a subspace of V which contains S. Moreoever, ny subspace of V which contains S as a subset must in fact contain all of span(S)
\end{theorem}
\begin{proof}
    To prove, 
    i) span(S) is a subspace of V 
    ii) span(S) $\subseteq V$ 

  Proving  span(S) $\subseteq V$,
    Let $\alpha_1v_1 + \alpha_2 v_2 \dots + \alpha_nv_n$ be any element in span(S), where $\alpha_i \in F, v_i \in S \text{ and } i = \{1,\dots n\}$ 
    Then
    \[
        \alpha_1v_1 + \alpha_2 v_2 + \dots + \alpha_nv_n \in span(S)
    \]
    Since, $\cdot$ and + are closed under V, $\alpha_1v_1 + \alpha_2 v_2 + \dots + \alpha_nv_n \in V$ \\
    $\therefore span(S) \in V$
\end{proof}

\section{Linear dependence and independence}
\begin{definition}[Linear dependence]
    Any collection S of vectors in a vector space V are linearly dependent, if we can find scalars \[
        \alpha_1, \alpha_2 \dots, \alpha_n \in \text{ F not all zero, such that}
    \]
    \begin{displaymath}
      \alpha_1 v_1 + \alpha_2v_2 + \dots \alpha_n v_n = 0  
    \end{displaymath}
\end{definition}
\section{Basis}
\begin{definition}[Basis]
    A basis in a vector space V is a set S of linearly independent vectors such that every vector in V is a linear combination of elements in S, i.e. S is the set of linearly independent vectors and is the spanning set of V.
\end{definition}
\begin{definition}[Dimension]
    The dimension of a finite dimensional vector space V is the number of elements in a basis of V.
\end{definition}
\begin{lemma}
    Let $\{v_1,v_2,\dots v_n\}$ be a basis for a vector space V. Then every vector v can be written in the form,
    \begin{equation}
        v = \alpha_1 v_1 + \alpha_2 v_2 + \dots + \alpha_nv_n 
        \label{eq:1}
    \end{equation}
\end{lemma}
\begin{proof}
    Let, 
    \begin{equation}
        v = \beta_1v_1+\beta_2 v_2 + \dots \beta_n v_n
        \label{eq:2}
    \end{equation}
    be another representation of v in $v_1,v_2 \dots v_n$
    \[
        \ref{eq:2} - \ref{eq:1}
    \]
    \[
        0 = (\beta_1 - \alpha_1)v_1 + (\beta_2 - \alpha_2)v_2 \dots + (\beta_n - \alpha_n)v_n
    \]    
    \[
        \therefore \alpha_1 = \beta_1, \alpha_2 = \beta_2 \dots \alpha_n = \beta_n
    \]
    ii) W is any subspace of V, which contains S. 

    Take any term v, which is a linear combination of $\alpha_i v_i$
    \[
        v = \alpha_1 v_1 + \alpha_2 v_2 \dots + \alpha_n v_n ,
    \]
    \[
       \therefore v \in span(S) \in W
    \]
\end{proof}
\begin{theorem}
    Let V be a vector space, and let S be a linearly independent subset of V. Let v be a vector which does not lie in S.
    a) If v lies in span(S) then the set $S \cup \{v\}$ is linearly dependent and span($S \cup \{v\}$ = span(S)
\end{theorem}


\end{document}
