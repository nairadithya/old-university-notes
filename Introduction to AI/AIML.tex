% Created 2024-07-26 Fri 13:20
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Adithya Nair}
\date{\today}
\title{Introduction To Artificial Intelligence And Machine Learning.}
\hypersetup{
 pdfauthor={Adithya Nair},
 pdftitle={Introduction To Artificial Intelligence And Machine Learning.},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.8)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Notes}
\label{sec:orgd99bf65}
\subsection{Iris Data Classification}
\label{sec:org38c2257}
\begin{verbatim}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

irisdata = pd.read_csv('iris.csv')

test, train = train_test_split(irisdata, train_size=0.8, test_size=0.2)

print(np.size(test))
print(np.size(train))
print(irisdata.describe())
\end{verbatim}
\subsection{Data Pre-Processing}
\label{sec:orgb14d0a6}
\subsubsection{Handling Missing Values (Imputation)}
\label{sec:orgbb9d1a2}
When the no. of missing values in a feature or on a whole in a dataset, is beyond a certain percentage. It might lead to wrong interpretations and might misguide the ML models.
Hence it is essential to handle the missing values.
\begin{enumerate}
\item CREATING A DATAFRAME
\label{sec:org9f2a8e9}
\begin{verbatim}
import pandas as pd
import numpy as np

# Load the Titanic dataset
df = pd.read_csv('titanic.csv')

# Display the first few rows of the dataset
print("First few rows of the dataset:")
print(df.head())
print(df.shape)
\end{verbatim}
This dataset is not complete, Cabin and Age have values that are unfilled. We can verify this here.
\begin{verbatim}

# Identify missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

\end{verbatim}
\item There are two main methods in dealing with missing values.
\label{sec:org8fc0ba3}
\begin{enumerate}
\item Dropping rows with missing values.
\item Filling the empty missing values with zeros.
\end{enumerate}
\begin{verbatim}
# Method 1: Drop rows with missing values
df_dropped = df.dropna()
print("\n METHOD 1 Shape of dataset after dropping rows with missing values:", df_dropped.shape)

# Method 2: Fill missing values with a specific value (e.g., 0)
df_filled_zeros = df.fillna(0)
print("\nMETHOD 2 Missing values filled with 0:")
print(df_filled_zeros.isnull().sum())

\end{verbatim}

This isn't exactly ideal. Deleting the rows loses too  much of the dataset, and filling with zeros does not work here when that might affect the correctness of the prediction.
So here we replace the values with the mean for numerical values and mode for categorical values.
\begin{enumerate}
\item {\bfseries\sffamily TODO} Look into other methods of imputation
\label{sec:orgd55def3}
\begin{verbatim}
# Method 3: Fill missing values with the mean (for numerical columns)
df['Age'].fillna(df['Age'].mean(), inplace=True)
print("\nMETHOD 3 Missing values in 'Age' column after filling with mean:")
print(df['Age'].isnull().sum())

# Method 4: Fill missing values with the most frequent value (mode)
df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)
print("\nMETHOD 4 Missing values in 'Embarked' column after filling with mode:")
print(df['Embarked'].isnull().sum())
\end{verbatim}
\end{enumerate}
\item Forward fill and Backward Fill
\label{sec:org9b9e830}
There are two better ways to fill the rows.
\begin{itemize}
\item Forward Fill - It iterates down the given data, and fills in missing values with the last value it saw.
\item Backward Fill - it iterates up the given data, and fills in missing values with the last value it saw.
\end{itemize}
\begin{verbatim}
# Method 5: Forward fill method
df_ffill = df.fillna(method='ffill')
print("\nMethod 5 Missing values handled using forward fill method:")
print(df_ffill.isnull().sum())

# Method 6: Backward fill method
df_bfill = df.fillna(method='bfill')
print("\nMethod 6 Missing values handled using backward fill method:")
print(df_bfill.isnull().sum())
print("*****************")
\end{verbatim}
\end{enumerate}
\subsubsection{Normalization}
\label{sec:org80f497e}
Used for multiple numerical features in the dataset, which belong to different ranges. I t would make ssense to normalize the data to a particular range.

Machine learning models tend to give a higher weightage to numerical attributres which have a larger value.

The solution is to normalize. Normalization reduces a given numerical feature into a range that is easier to manage as well as equate with other numerical features.
\begin{enumerate}
\item Types Of Normalization
\label{sec:org4a01d6a}
\begin{itemize}
\item MinMaxScaler - all data points are brought to the range \([0,1]\)
\item Z-score - Data points are converted in such a way that the mean becomes 0 and the standard deviation is 1.
\end{itemize}
\begin{enumerate}
\item NORMALISING A SET OF VALUES USING MIN MAX NORMALIZATION
\label{sec:org6baf52a}
\begin{verbatim}
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Example usage:
data = np.array([2, 5, 8, 11, 14]).reshape(-1, 1)  # Reshape to 2D array for scaler

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Apply Min-Max normalization
normalized_data = scaler.fit_transform(data)

# Flatten the normalized data to 1D array
normalized_data = normalized_data.flatten()

print(normalized_data)
\end{verbatim}
\item NORMALISING A SET OF VALUES USING Z-SCORE NORMALIZATION
\label{sec:org0d79717}
\begin{verbatim}
import numpy as np
from sklearn.preprocessing import StandardScaler

# Example usage:
data = np.array([2, 5, 8, 11, 14]).reshape(-1, 1)  # Reshape to 2D array for scaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Apply Z-score normalization
normalized_data = scaler.fit_transform(data)

# Flatten the normalized data to 1D array
normalized_data = normalized_data.flatten()

print(normalized_data)
\end{verbatim}
\item NORMALIZING CERTAIN COLUMNS IN THE DATAFRAME
\label{sec:org9b24003}
\begin{verbatim}
# Initialize the MinMaxScaler
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

# List of columns to be normalized
columns_to_normalize = ['Age', 'Fare']

# Apply Min-Max normalization
df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])

print("\nDataFrame after Min-Max normalization:")
print(df)
\end{verbatim}
\end{enumerate}
\end{enumerate}
\subsubsection{SAMPLING}
\label{sec:org19694b4}
\begin{enumerate}
\item RANDOM SAMPLING
\label{sec:org116888e}
\begin{verbatim}
import random

# Sample data
population = list(range(1, 101))  # Population from 1 to 100
sample_size = 10  # Size of the sample

# Simple random sampling
sample = random.sample(population, sample_size)
print("Simple Random Sample:", sample)
\end{verbatim}
\item STRATIFIED SAMPLING
\label{sec:org326853d}
\begin{verbatim}
import random

# Sample data with strata
strata_data = {
    'stratum1': [1, 2, 3, 4, 5],
    'stratum2': [6, 7, 8, 9, 10],
}

# Sample size per stratum
sample_size_per_stratum = 2

# Stratified sampling
sample = []
for stratum, data in strata_data.items():
    stratum_sample = random.sample(data, sample_size_per_stratum)
    sample.extend(stratum_sample)

print("Stratified Sample:", sample)
\end{verbatim}
\item Systematic Sampling
\label{sec:org11fe5b8}
\begin{verbatim}
# Sample data
data = list(range(1, 101))  # Data from 1 to 100
n = 5  # Every nth data point to be included in the sample

# Systematic sampling
sample = data[::n]
print("Systematic Sample:", sample)
\end{verbatim}


\begin{verbatim}
import random

# Sample data with clusters
clusters = {
    'cluster1': [1, 2, 3],
    'cluster2': [4, 5, 6],
    'cluster3': [7, 8, 9],
}

# Number of clusters to sample
clusters_to_sample = 2

# Cluster sampling
selected_clusters = random.sample(list(clusters.keys()), clusters_to_sample)
print("chosen clusters ", selected_clusters)
sample = []
for cluster in selected_clusters:
    sample.extend(clusters[cluster])

print("Cluster Sample:", sample)
\end{verbatim}
\end{enumerate}
\subsubsection{BINNING}
\label{sec:org28726de}
\begin{verbatim}
import pandas as pd

# Assuming 'df' is your DataFrame containing the dataset
budget_bins = [0, 10, 20, float('inf')]  # Define your budget bins
budget_labels = ['Low Budget', 'Medium Budget', 'High Budget']  # Labels for the bins

df['BudgetBin'] = pd.cut(df['Budget'], bins=budget_bins, labels=budget_labels)
\end{verbatim}

\begin{verbatim}
df.head(10)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{ce9e3744fc6a3bb5507b36f5f296493e45f8052b.png}
\end{center}

\begin{verbatim}
collection_bins = [0, 20, 40, 60, float('inf')]  # Define your collection bins
collection_labels = ['Low Collection', 'Medium Collection', 'High Collection', 'Very High Collection']  # Labels for the bins

df['CollectionBin'] = pd.cut(df['BoxOfficeCollection'], bins=collection_bins, labels=collection_labels)
df.head(10)
\end{verbatim}

\begin{center}
\includegraphics[width=.9\linewidth]{6d1b533234c55700921c51ec2564d6917a09ab42.png}
\end{center}
\subsection{Supervised Learning}
\label{sec:org3d967c2}
\end{document}
