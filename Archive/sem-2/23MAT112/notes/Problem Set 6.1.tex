\documentclass{report}

\input{preamble}
\title{\Huge{23MAT112}\\ Class Notes}
\author{\huge{Adithya Nair}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\tableofcontents

\pagebreak

\chapter{Eigenvalues And Eigenvectors}
\section{Introduction} % (fold)
\label{sec:Introduction}
Reference - \textbf{Learning from Data, Gilbert Strang as well as Chapter 6 of Introduction To Linear Algebra}
\begin{align*}
	A \vec{x} = \lambda \vec{x}
\end{align*}

This equation is a mathematical way of expressing the idea that there's some vector $\vec{x}$ that does not change in direction, but only changes in size or magnitude by some factor $\lambda$.


\begin{definition}{Eigenvector}
   A vector that does not change in direction after a linear transformation.
\end{definition}
\begin{definition}{Eigenvalue}
The scalar factor by which an eigenvector changes after a linear transformation	
\end{definition}
\section{Example Matrices} % (fold)
\label{sec:Example Matrices}
\subsection{Projection Matrices}
A reminder, projection matrices are used to bring vectors outside the column space of a given matrix A to the column space of A.
\[
	Ax = b
\]
\section*{3 Cases}
When $\vec{x}$ is in the column space,
\[
	Px = \lambda x, where \\ \lambda = 1
\]
When $\vec{x}$ is out of the column space,
\[
   \text{It is not an eigen vector}
\]
When $\vec{x}$ is orthogonal to the column space,
\[
	Px = \lambda x, where \lambda = 0
\]
% section Example Matrices (end)
\subsection{Permutation Matrix}
\[
	\begin{bmatrix}
	   0 & 1 \\
	   1 & 0 \\
	\end{bmatrix}
	\begin{bmatrix}
	x_1 \\
	x_2 \\
	\end{bmatrix}
	= 
\begin{bmatrix}
   x_2 \\ 
   x_1 \\
\end{bmatrix}
\]
The eigenvectors for this matrix is:
\[
	\begin{bmatrix}
	1 \\
	1 \\
	\end{bmatrix}
	and 
	\begin{bmatrix}
      -1 \\
      1 \\
	\end{bmatrix}
\]
\subsection{Rotation matrix}
\[
	R = 
	\begin{bmatrix}
		cos \theta & - sin \theta \\
		sin \theta & cos \theta \\
	\end{bmatrix}
\]
For $\theta= \frac{\pi}{2}$
\[
	R = 
	\begin{bmatrix}
		 0 & -1 \\
		 1 & 0 \\
	\end{bmatrix}
\]
The calculated eigenvalues for this matrix is $\pm i$
\begin{note}
	A matrix is the representation of a linear transformation in a given basis
	\[
		L(\alpha a + \beta b) = \alpha L(a) + \beta L(b)
	\]
\end{note}
\begin{question}
	B is $3 \times3$ matrix with eigenvalues $\lambda = 0,1,2$.
	 \begin{enumerate}
		\item What is the rank of B?
		\item What is the determinant of B?  
	\end{enumerate}
	\textbf{Ans. 1. - } The rank of B is 2. There is one distinct eigenvalue which is 0. This means that the nullspace is one-dimensional. The nullspace is (n-r) dimensions, therefore the rank of the matrix is 2. \\
	\textbf{Ans. 2. - } The determinant is 0, since there exists an eigenvalue which is 0, which means that 
	\[
	   det(A - \lambda I) = 0, \text{where } \lambda \text{ is 0}
	\]
\end{question}
% section Introduction (end)
\section{Diagonalizable Matrices} % (fold)
\label{sec:Diagonalizable Matrices}

\begin{definition}[Diagonalizable Matrices]
	Let A and B be two square matrices of size $n \times n$. We say that A and B are similar if there is an invertible matrix of the same size P such that: 
	\[
	   A = PBP^{-1}
	\]
	Then we can say that A is \textbf{diagonalizable} if A is similar to a diagonal matrix D
\end{definition}
\begin{lemma}
   Suppose that A and B are two $n \times n$ matrices and P is an invertible matrix, such that $A = PBP^{-1}.$ Then, $A^{n} - PB^{n}P^{-1}$ 
\end{lemma}
\begin{proof}
	Using the principle of mathematical induction.\\
	We are given, $A = PBP^{-1}$\\
	to show, $A^{n} = PB^{n}P^{-1}$\\
	\textbf{Base step - } $n = 1, A^{1} = P^1 B^1 P^{-1}$, which is true \\
	\textbf{Induction step - } Supppose $A^{n} = PB^{n}P^{-1}$. We need to show that $A^{n+1} = PB^{n+1}P^{-1}$\\
	\[
	   A^{n+1} = A \dot A^{n}
	\]
	\[
	   = (PBP^{-1})(PB^n P^{-1})
	\]
	\[
	   = PBB^nP^{-1} = PB^{n+1}P^{-1}
	\]
\end{proof}

\begin{theorem}
   Let A be an $n \times n$ matrix and let $v_1,v_2,\dots,v_k$ be eigenvectors of A with distinct eigenvalues $\lambda_1, \lambda_2,\dots\lambda_k$. Then $v_1,v_2\dots v_k$ are independent. In particular, if k = n, then $v_1,v_2\dots v_k$ are a basis of eigenvectors for $\mathbb{R}^n$
\end{theorem}
\begin{proof}
	Suppose $v_1,v_2\dots v_n$ are dependent such that $\exists r_i$  such that,
	\begin{equation}
	    \sum_{i=1}^{k} r_i v_i = 0
		\label{eq:1}
	\end{equation}
	Assume that k is minimal with this property and $r_k$ is all non-zero,
	\[
		A.0 = r_1 A v_1 + r_2 A v_2 \dots + r_k A v_k
	\]
	\begin{equation}
	0 = r_1\lambda_1 v_1 + r_2 \lambda_2 v_2 \dots r_k \lambda_k v_k = 0	
		\label{eq:2}
	\end{equation}
	\begin{equation}
	   \lambda_k \times \ref{eq:1}: r_1 \lambda_k v_1 + \dots + r_k \lambda_k v_k = 0	
		\label{eq:3}
	\end{equation}
	\ref{eq:3} - \ref{eq:2}
	\[
	   r_1(\lambda_k - \lambda_1)v_1 + \dots + r_{k-1}(\lambda_k - \lambda_{k-1})v_{k-1} = 0
	\]
	$\lambda_k - \lambda_i$ is non-zero(distinct eigenvalues)\\
	This forms a linear combination of vectors $v_1,v_2\dots v_{k-1}$ that equate to zero. \\
	This contradicts the assumption that $v_1,v_2\dots v_{k-1}$ is the minimal set which is dependent. \\
	Therefore the eigenvectors are independent.
     \end{proof}
\begin{theorem}
   Let A be a n $\times$ n matrix, Then A is diagonalizable if and only if we can find a basis $v_{1},\cdots, v_{n}$ of eigen vectors for $R^{n}$. In this case,
   \begin{equation}
      A = PDP^{1}	
   \end{equation}
   where P is the matrix whose eigenvectors $v_{1},\cdots, v_{n}$ and D is the diagonal matrix whose diagonal entries are the corresponding eigenvalues $\lambda_{1}, \cdots, \lambda_{n}$
\end{theorem}
\begin{proof}
   \[
   Av_{i} = (PDP^{-1}v_{i})
   \]
where P is the matrix with column vectors $v_{1},v_{2} \cdots v_{n}$ and D is a diagonal matrix with the entries $\lambda_{1},\cdots, \lambda_{n}$
\[
Av_i = (PDP^{-1})P \hat{e}_{i} \\
\]
\[
   Av_{i} = PD\hat{e}_i
\]
\[
Av_{i} = P\lambda_{i} \hat{e}_{i} = \lambda_{i} P \hat{e}_{i} = \lambda_i v_{i}
\]
This proves that $v_{i}$ is the eigenvector of A, and that $\lambda_{i}$ is the corresponding eigenvalue.
Because $P^{-1}$ exists, $v_{1},\cdots,v_{n}$ are independent, based on the theorem proven earlier, this is the basis for $R^{n}$ \\
\textbf{Part b}
Suppose $v_{1},\cdots, v_{n}$ are an eigenvector basis with corresponding eigenvalues $\lambda_{1},\cdots,\lambda_{n}$
Suppose that P is the matrix with column vectors $v_{1},\cdots,v_{n}$ \[
   \text{Let} D = P^{-1}AP 
\]
\[
   D \hat{e}_{i} = (P^{-1}AP)\hat{e}_i
\]
\[
   = P^{-1} A v_{i} = \lambda_{i} P^{-1}v_{i} = \lambda_{i} \hat{e}_{i}
\]
Thus D is the diagonal matrix with the diagonal entries $\lambda_{i}$
\end{proof}
% section Diagonalizable Matrices (end)
\section{Symmetric Matrices} % (fold)
\label{sec:Symmetric Matrices}
\[
	A = A^T
\]
A symmetric matrix has real eigenvalues and orthogonal eigenvectors.
\begin{example}
	\[
	   A =
	   \begin{bmatrix}
	      3 & 1\\
	      1 & 3\\
	   \end{bmatrix}
	\]
	The eigenvalues for the matrix is 2 and 4
	The eigenvectors are:
	\[
	   \vec{v} = 
	   \begin{bmatrix}
	   	1 \\
		-1
	   \end{bmatrix}
	   and
	   \begin{bmatrix}
	   	1 \\
		1 \\
	   \end{bmatrix}
	\]
	These eigenvectors are orthogonal.
\end{example}
\begin{definition}{Hermitian Matrices}
	A complex matrix in which all the entries in the given matrix are equal to their corresponding conjugate transposes.
\end{definition}
\begin{theorem}
   The eigenvalues for a symmetric matrix are real.
\end{theorem}
\begin{proof}
   The claim:
	\[
	For \\	Ax = \lambda x, A = A^T \\
	\]
	\[
		\lambda \in R
	\]
	Take:
	\[
		A x = \lambda x
	\]
	And the complex conjugate:
	\[
	   \overline{A}\overline{x} = \overline{\lambda} \overline{x} \indent  (\overline{A} = A)(real \\ matrix)
	\]
\begin{equation}
	   (\overline{A}\overline{x})^T = (\overline{\lambda} \overline{x})^T \iff \overline{x}^T A^T = \overline{\lambda} \overline{x}^T
	\label{eq:Conjugate}
\end{equation}	
\ref{eq:Conjugate} $\times$ x
\[
   \overline{x}^T A^T x = \lambda \overline{x^T} x \iff \overline{x}^T A x = \overline{\lambda} \overline{x}^T x \iff \overline{x}^T \lambda x = \overline{\lambda} \overline{x}^T x \iff \lambda = \overline{\lambda}
\]
Then $\lambda$ is real, when $\overline{x}^T x \neq 0$
\end{proof}
\begin{lemma}
   Let A be a symmetric matrix. If v and w are eigenvectors with distinct eigenvalues $\lambda \& \mu$ then v \& w are orthogonal.
\end{lemma}

\begin{proof}
	\[
		Av .  w = (Av)^T w = v^T A^Tw = v^T Aw = v . Aw
	\]
	\[
	\iff \lambda v . w = \mu v . w \iff (\lambda - \mu) v.w = 0
	\]
	\[
		\iff v . w  = 0
	\]
\end{proof}
What this means is that, when A is symmetric
\[
   A = PDP^{-1} = PDP^T \indent (P^{-1} = P^T \text{for orthogonal matrices})
\]
% section Symmetric Matrices (end)
\begin{theorem}
   Let A be a symmetric matrix. Then we can find a diagonal matrix D and an orthogonal matrix P such that,
\[
   A = PDP^T
\]	
In particular, every symmetric matrix is diagonalisable.
\end{theorem}
\end{document}
