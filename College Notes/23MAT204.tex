% Created 2024-08-06 Tue 15:47
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\input{preamble}
\author{Adithya Nair}
\date{\today}
\title{23MAT204}
\hypersetup{
 pdfauthor={Adithya Nair},
 pdftitle={23MAT204},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.8)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\part{Mathematics}
\label{sec:orga3a67f7}
\chapter{Revision Linear Algebra}
\label{sec:orgb79fdf4}

Every matrix satisfies its own characteristic matrix.

What this means is
\[\begin{aligned}
    \lambda^3 - 2\lambda^2 + \lambda -4 &= 0 \\ 
    A^3 - 2A^2 + A -4I &= 0 \\  
    A^2 - 2A + I - 4A^{-1} &= 0 \\
    A^{-1} &= \frac{1}{4}[A^2 - 2A + I] \\
    X = A^{-1}b &= \frac{1}{4}[A^2 - 2A + I]b
\end{aligned}\] There are some implications behind this. Here's a
problem, generate a random 3x3 matrix, find the ranks of
\((A- \lambda_1 I)\) and then \((A-\lambda_1 I)(A-\lambda_2 I\)

What you will see is that the rank reduces upon multiplying roots of the
characteristic equation
\section{Ways To Calculate Whether A Matrix Is Positive Definite}
\label{sec:orgc0d9a74}
\begin{enumerate}
\item The minors are positive then negative alternating.
\item The eigenvalues are positive.
\end{enumerate}
\section{Large Eigenvalue Computation}
\label{sec:org232a1fa}
Large eigenvalues are computed by using a matrix multiplication method.
This method involves:
\section{Specral Decomposition}
\label{sec:orge2eb8f7}
\[S = Q \Lambda Q^T = \lambda_1 q_1 q_1^T + \lambda_2 q_2 q_2^T + \cdots + \lambda_n q_n q_n^T\]

This allows us to represent a matrix by the sum of n rank 1 matrices.
This is used in square symmetric matrices.

This is the basis behind Principal Component Analysis.
\section{Singular Value Decomposition}
\label{sec:org51126a8}
For rectangular matrices.
\[A = U \Sigma V^T  = \sigma_1 u_1 v_1^T + \sigma_2u_2v_2^T + \cdots + \sigma_r u_r v_r^T\]

This is mainly used for getting useful properties about the matrix
without needing to perform significant operations on this matrix, such
as the orthogonality.

Singular values are the eigenvalues of a matrix which are \textbf{non-zero}

So a rectangular matrix \(m \times n\) would have \(AA^T\) which is
\(m \times m\) and \(A^TA\) which is \(n \times n\). The number of
singular values is whether \(m\) or \(n\) is lower.

The trace of \(A^TA\) equal to the sum of all \(a_{ij}^2\) The trace is
the sum of all eigenvalues of \(A^T A\), and For \(A_{m \times n}\)
that's the sum of the eigenvalues squared.

This is known as the \textbf{Frobenius Norm}

This method can also be used to generate orthonormal bases for the 4
Fundamental Subspaces
\section{The Geometry of SVD}
\label{the-geometry-of-svd}
Since we have an orthogonal matrix, diagonal and orthogonal matrix\ldots{}
It's a rotation step followed by a stretching step, finally ended by
another rotation step.
\section{Polar Decomposition}
\label{polar-decomposition}
This decomposition is a special case of the Singular Value
Decomposition. Where you decompose the elements into the polar form with
an orthogonal matrix and a positive semi-definite matrix.

For 2D, \[\vec{x} = r(\cos(\theta) + sin(\theta))\]

\[\begin{aligned}
    A &= U \Sigma V^T \\ 
      &= U(V^TV) \Sigma V^T \\
      &= (UV^T)(V\Sigma V^T)\\
      &= Q \times S \\
\end{aligned}\]

Here S is the scaling matrix, and Q is the orthogonal matrix.

Although my assumptions might be wrong, this might be a way to generate
rotation matrices for higher-dimension spaces. Might be useful in
exploring the semantic spaces of LLMs.
\section{Principal Component Analysis}
\label{principal-component-analysis}
Why do we use normalization while working with data? We use
normalization to ensure that the units are eliminated while working with
that given data. \textbf{This normalization is done by subtracting by the
average and divide by the standard deviation.}

Matrix where the row and columns sums are equal.
\section{Norms Of Vectors And Matrices}
\label{norms-of-vectors-and-matrices}
Norms are a way to measure the size of a vector/matrix. The norm of a
vector which calculates the magnitude is known as the \(L^2\) norm or
the Euclidean norm \(\|v\|_2\). This number gives us the length of the
vector.

In general, the \(\|u\|_p\) or the p-norm of a vector is
\([|u_1|^p + |u_2|^p + \cdots + |u_n|^p]^{\frac{1}{p}}\).

Now, the \(L_1\) norm would be the sum of the components of the vector.
This is the sum of the projections to each corresponding axis.

The \(L_3\) norm would be
\([|u_1|^3 + |u_2|^3 + \cdots + |u_n|^3]^{\frac{1}{3}}\)

The \(L_\infty\) norm would be
\([|u_1|^\infty + |u_2|^\infty + \cdots + |u_n|^\infty]^{\frac{1}{\infty}}\).
This returns the maximum vector component of the vector.

Let's take the equation, \(\|v\|_1 = 1\) $\backslash$[\begin{aligned}
    x + y \&= 1 \\
    x - y \&= 1 \\
\begin{itemize}
\item x + y \&= 1 \\
\end{itemize}
    -x - y = 1 \\
\end{aligned}$\backslash$]

\label{fig:l1eq1}
\url{./figures/l1eq1}

The S-norm of a vector \(\vec{x}\) is \(\vec{x}^T S \vec{x}\). When S is
a symmetric positive definite matrix, this S-norm is known as the energy
of vector \(\vec{v}\)

There are three types of Matrix norms:

\begin{enumerate}
\item Spectral Norm

\item Frobenius Norm =
\(\sqrt{\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_r^2}\)

\item Nuclear Norm
\end{enumerate}
\begin{enumerate}
\item Spectral Norm
\label{spectral-norm}
We know that the vector norm for a vector \(\vec{x}\) is nothing but
\(\vec{x}^T \vec{x}\). We take this property.

\[\begin{aligned}
    Max \|A\|_2^2 &= Max \frac{\|Ax\|_2^2}{\|x\|_2^2} \\
              &= Max \frac{x^TA^TAx}{x^Tx} \\
              &= Max \{\lambda_i(S)\} = \lambda_1 = \sigma_1^2
\end{aligned}\]
\item Frobenius Norm
\label{frobenius-norm}
The Frobenius norm for a matrix M, \(\begin{bmatrix}
    a_{11} & a_{12}\\
a_{21} & a_{22} \\
\end{bmatrix}\) is the equation
\(\sqrt{a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2}\)
\item Nuclear Norm
\label{nuclear-norm}
The nuclear norm is the sum of the singular values of a matrix A.

For an identity matrix,

\begin{itemize}
\item The Spectral Norm is 1

\item The Frobenius Norm is \(\sqrt{n}\)

\item The Nuclear Norm is \({n}\)
\end{itemize}

For an orthogonal matrix,

\begin{itemize}
\item The Spectral Norm is 1

\item The Frobenius Norm is \(\sqrt{n}\)

\item The Nuclear Norm is \({n}\)
\end{itemize}
\end{enumerate}
\section{Best Low Rank Matrix}
\label{best-low-rank-matrix}
We say that a matrix is the best approximation of another matrix, based
on the Frobenius Norm.. For a singular value decomposition
\(A = U\Sigma V^T\), if we assume that the singular values are arranged
in descending order\ldots{} We can select the singular value range where the
values are significant contributors to the final matrix.

We can then reduce the size of \(U, \Sigma\) and \(V^T\) into a smaller
matrix B, based on the number of singular values chosen which would give
the best approximation.

Let \(A = U\Sigma V^T\) where
\(\Sigma: \sigma_1 \geq \sigma_2 \geq \cdots \sigma_n\), then B =
\(U_{m\times m} \Sigma V^T{n \times n}\) is a best rank-k approx. to A.
Where, S is a diagonal matrix of \(n \times n\) where
\(s_i = \sigma_i (i = 1\cdots k)\) else \(s_i = 0\), by best B is a
solution to \(min_B \|A - B\|F\) where rank(B) = k
\chapter{Multivariable Optimization}
\label{sec:org7802645}

We minimize \(f(x_1,x_2,\cdots,x_3)\)
\part{PCA IEEE Report}
\label{sec:org878429a}
\chapter{References}
\label{sec:org36dfa8f}
\begin{itemize}
\item \url{https://arxiv.org/pdf/2403.15112}
\item \url{https://arxiv.org/pdf/2402.15527}
\item \url{https://medium.com/@anabelenmanjavacas/dimensionality-reduction-and-pca-23dbd7d6f367}
\item \url{https://ieeexplore.ieee.org/document/10511242}
\end{itemize}
\chapter{What is PCA?}
\label{sec:org4cfca64}
In a nutshell, what Principal Component Analysis does is reduce the dimensionality of a set of data points by linearly projecting them onto a lower-dimensional space where the reconstruction error is as minimal as possible.
\end{document}
