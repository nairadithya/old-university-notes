% Created 2024-08-18 Sun 09:53
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\input{preamble}
\author{Adithya Nair}
\date{\today}
\title{23MAT204}
\hypersetup{
 pdfauthor={Adithya Nair},
 pdftitle={23MAT204},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 29.4 (Org mode 9.8)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\part{Mathematics}
\label{sec:orgb07d2f5}
\chapter{Revision Linear Algebra}
\label{sec:orgce265df}

Every matrix satisfies its own characteristic matrix.

What this means is
\[\begin{aligned}
    \lambda^3 - 2\lambda^2 + \lambda -4 &= 0 \\ 
    A^3 - 2A^2 + A -4I &= 0 \\  
    A^2 - 2A + I - 4A^{-1} &= 0 \\
    A^{-1} &= \frac{1}{4}[A^2 - 2A + I] \\
    X = A^{-1}b &= \frac{1}{4}[A^2 - 2A + I]b
\end{aligned}\] There are some implications behind this. Here's a
problem, generate a random 3x3 matrix, find the ranks of
\((A- \lambda_1 I)\) and then \((A-\lambda_1 I)(A-\lambda_2 I\)

What you will see is that the rank reduces upon multiplying roots of the
characteristic equation
\section{Ways To Calculate Whether A Matrix Is Positive Definite}
\label{sec:orga87040c}
\begin{enumerate}
\item The minors are positive then negative alternating.
\item The eigenvalues are positive.
\end{enumerate}
\section{Large Eigenvalue Computation}
\label{sec:org318ca59}
Large eigenvalues are computed by using a matrix multiplication method.
This method involves:
\section{Specral Decomposition}
\label{sec:org9cd488c}
\[S = Q \Lambda Q^T = \lambda_1 q_1 q_1^T + \lambda_2 q_2 q_2^T + \cdots + \lambda_n q_n q_n^T\]

This allows us to represent a matrix by the sum of n rank 1 matrices.
This is used in square symmetric matrices.

This is the basis behind Principal Component Analysis.
\section{Singular Value Decomposition}
\label{sec:org0a0a49f}
For rectangular matrices.
\[A = U \Sigma V^T  = \sigma_1 u_1 v_1^T + \sigma_2u_2v_2^T + \cdots + \sigma_r u_r v_r^T\]

This is mainly used for getting useful properties about the matrix
without needing to perform significant operations on this matrix, such
as the orthogonality.

Singular values are the eigenvalues of a matrix which are \textbf{non-zero}

So a rectangular matrix \(m \times n\) would have \(AA^T\) which is
\(m \times m\) and \(A^TA\) which is \(n \times n\). The number of
singular values is whether \(m\) or \(n\) is lower.

The trace of \(A^TA\) equal to the sum of all \(a_{ij}^2\) The trace is
the sum of all eigenvalues of \(A^T A\), and For \(A_{m \times n}\)
that's the sum of the eigenvalues squared.

This is known as the \textbf{Frobenius Norm}

This method can also be used to generate orthonormal bases for the 4
Fundamental Subspaces
\section{The Geometry of SVD}
\label{the-geometry-of-svd}
Since we have an orthogonal matrix, diagonal and orthogonal matrix\ldots{}
It's a rotation step followed by a stretching step, finally ended by
another rotation step.
\section{Polar Decomposition}
\label{polar-decomposition}
This decomposition is a special case of the Singular Value
Decomposition. Where you decompose the elements into the polar form with
an orthogonal matrix and a positive semi-definite matrix.

For 2D, \[\vec{x} = r(\cos(\theta) + sin(\theta))\]

\[\begin{aligned}
    A &= U \Sigma V^T \\ 
      &= U(V^TV) \Sigma V^T \\
      &= (UV^T)(V\Sigma V^T)\\
      &= Q \times S \\
\end{aligned}\]

Here S is the scaling matrix, and Q is the orthogonal matrix.

Although my assumptions might be wrong, this might be a way to generate
rotation matrices for higher-dimension spaces. Might be useful in
exploring the semantic spaces of LLMs.
\section{Principal Component Analysis}
\label{principal-component-analysis}
Why do we use normalization while working with data? We use
normalization to ensure that the units are eliminated while working with
that given data. \textbf{This normalization is done by subtracting by the
average and divide by the standard deviation.}

Matrix where the row and columns sums are equal.
\section{Norms Of Vectors And Matrices}
\label{norms-of-vectors-and-matrices}
Norms are a way to measure the size of a vector/matrix. The norm of a
vector which calculates the magnitude is known as the \(L^2\) norm or
the Euclidean norm \(\|v\|_2\). This number gives us the length of the
vector.

In general, the \(\|u\|_p\) or the p-norm of a vector is
\([|u_1|^p + |u_2|^p + \cdots + |u_n|^p]^{\frac{1}{p}}\).

Now, the \(L_1\) norm would be the sum of the components of the vector.
This is the sum of the projections to each corresponding axis.

The \(L_3\) norm would be
\([|u_1|^3 + |u_2|^3 + \cdots + |u_n|^3]^{\frac{1}{3}}\)

The \(L_\infty\) norm would be
\([|u_1|^\infty + |u_2|^\infty + \cdots + |u_n|^\infty]^{\frac{1}{\infty}}\).
This returns the maximum vector component of the vector.

Let's take the equation, \(\|v\|_1 = 1\) $\backslash$[\begin{aligned}
    x + y \&= 1 \\
    x - y \&= 1 \\
\begin{itemize}
\item x + y \&= 1 \\
\end{itemize}
    -x - y = 1 \\
\end{aligned}$\backslash$]

\label{fig:l1eq1}
\url{./figures/l1eq1}

The S-norm of a vector \(\vec{x}\) is \(\vec{x}^T S \vec{x}\). When S is
a symmetric positive definite matrix, this S-norm is known as the energy
of vector \(\vec{v}\)

There are three types of Matrix norms:

\begin{enumerate}
\item Spectral Norm

\item Frobenius Norm =
\(\sqrt{\sigma_1^2 + \sigma_2^2 + \cdots + \sigma_r^2}\)

\item Nuclear Norm
\end{enumerate}
\begin{enumerate}
\item Spectral Norm
\label{spectral-norm}
We know that the vector norm for a vector \(\vec{x}\) is nothing but
\(\vec{x}^T \vec{x}\). We take this property.

\[\begin{aligned}
    Max \|A\|_2^2 &= Max \frac{\|Ax\|_2^2}{\|x\|_2^2} \\
              &= Max \frac{x^TA^TAx}{x^Tx} \\
              &= Max \{\lambda_i(S)\} = \lambda_1 = \sigma_1^2
\end{aligned}\]
\item Frobenius Norm
\label{frobenius-norm}
The Frobenius norm for a matrix M, \(\begin{bmatrix}
    a_{11} & a_{12}\\
a_{21} & a_{22} \\
\end{bmatrix}\) is the equation
\(\sqrt{a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2}\)
\item Nuclear Norm
\label{nuclear-norm}
The nuclear norm is the sum of the singular values of a matrix A.

For an identity matrix,

\begin{itemize}
\item The Spectral Norm is 1

\item The Frobenius Norm is \(\sqrt{n}\)

\item The Nuclear Norm is \({n}\)
\end{itemize}

For an orthogonal matrix,

\begin{itemize}
\item The Spectral Norm is 1

\item The Frobenius Norm is \(\sqrt{n}\)

\item The Nuclear Norm is \({n}\)
\end{itemize}
\end{enumerate}
\section{Best Low Rank Matrix}
\label{best-low-rank-matrix}
We say that a matrix is the best approximation of another matrix, based
on the Frobenius Norm.. For a singular value decomposition
\(A = U\Sigma V^T\), if we assume that the singular values are arranged
in descending order\ldots{} We can select the singular value range where the
values are significant contributors to the final matrix.

We can then reduce the size of \(U, \Sigma\) and \(V^T\) into a smaller
matrix B, based on the number of singular values chosen which would give
the best approximation.

Let \(A = U\Sigma V^T\) where
\(\Sigma: \sigma_1 \geq \sigma_2 \geq \cdots \sigma_n\), then B =
\(U_{m\times m} \Sigma V^T{n \times n}\) is a best rank-k approx. to A.
Where, S is a diagonal matrix of \(n \times n\) where
\(s_i = \sigma_i (i = 1\cdots k)\) else \(s_i = 0\), by best B is a
solution to \(min_B \|A - B\|F\) where rank(B) = k
\chapter{Multi variable Optimization}
\label{sec:orgab95e67}

We minimize \(f(x_1,x_2,\cdots,x_3)\).
\section{Contour Curves}
\label{sec:org43295ad}
\section{Multi variable Calculus}
\label{sec:org17d301f}
The points where \(\nabla f = \vec{0}\) are called the stationary points. The Hessian matrix should be positive definite for a minima and negative definite for a maxima. A point where there is no change, is known as a saddle point.

\begin{align*}
f(x,y) = x^3 + y^3 + 2x^2 + 4y^2 + 6 \\
\frac{\partial f}{\partial x} = 3x^2 + 4x \\
\frac{\partial^2 f}{\partial x} = 6x + 4 \\
\frac{\partial f}{\partial y} = 3y^2 + 8y\\
\frac{\partial f}{\partial y\partial x} = 0\\
\frac{\partial^2 f}{\partial y} = 6y + 8\\
x = 0,\frac{-4}{3} \\
y = 0,\frac{-8}{3} \\
\end{align*}
\section{Newton's Method}
\label{sec:orgfe43c3b}
Newton's Method is a numerical method to find a minimum of a function.

Iterative formula:

\[
x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}
\]

Find the minimum of \(f(x) = x^2 + \frac{54}{x}\) using Newton's method.

Newton's method can be practically done for all functions by evaluating the first and second derivatives numerically and then apply the formula

$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \text{to find root of $f(x)$ = 0}$$
$$x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} \text{to find minimum of $f(x)$(root of $f'(x) = 0)}$$

Evaluation of first and second derivatives numerically:

$$f'(a) = \frac{f(a+\Delta a)-f(a-\Delta a)}{2 \Delta a}$$
$$f''(a) = \frac{f(a+\Delta a)- 2f(a) + f(a-\Delta a)}{(\Delta a)^2}$$
\section{How solution to a linear system can be found as a solution of an optimization problem.}
\label{sec:orgf96e5cb}
Find an objective function whose solution is specified as \(\vec{x} : A\vec{x} = b\)

Take for example, \(\frac{1}{2}x^T A x - b^T x + c\), \(x \in R^n, A = A^T\)
\section{Numerical Algorithm Of Gauss-Jacobi Method}
\label{sec:org619ae60}

Input A = [a\textsubscript{ij}], X0 = x\textsuperscript{(0)}, tolerance TOL, maximum number of iterations,

\(Ax = b\)

We separate A,
A = D - L - U

$$(D-L-U)x = b$$
$$(Dx= b + (L+U)x$$
$$(x= D^{-1}b + D^{-1}(L+U)x$$

$$x^{(k)} = Tx^{x^{(k-1)}} + c$$


The code to implement the method:
\begin{verbatim}
A = [5,-2,3;-3,9,1;2,-1,-7];
b = [-1;2;3];

n = 100; % No. of iterations
D = diag(diag(A));
L = -tril(A,-1);
U = -triu(A,1);

X = zeros(size(A)(1),n);
T = inv(D)*(L+U);
c = inv(D)*b;

for i = 2:n
  X(:,i) = T*X(:,i-1) + c;
  if(X(:,i) == X(:,i-1))
    i-1
    break;
  end
end
\end{verbatim}
\section{Gauss-Siedel Iteration Method}
\label{sec:org4655ec5}
The iterative method is somewhat similar to <Gauss-Jacobi method>.

The only difference is that the new values are computed by already existing new values.

\begin{verbatim}
A = [5,-2,3;-3,9,1;2,-1,-7];
b = [-1;2;3];

n = 100; % No. of iterations
D = diag(diag(A));
L = -tril(A,-1);
U = -triu(A,1);

X = zeros(size(A)(1),n);
T = inv(D-L)*U;
c = inv(D-L)*b;

for i = 2:n
  X(:,i) = T*X(:,i-1) + c;
  if(X(:,i) == X(:,i-1))
    display("The iteration it converges at:")
    i-1
    break;
  end
end
\end{verbatim}

$$(D-L)x^{k} = Ux^{k-1} + b$$
$$x^{k} = (D-L)^{-1} \vec{b} + (D-L)^{-1}(U \vec{x})$$

If the matrix is 'diagonally dominant', where the magnitude of the diagonal elements should be greater than the sum of the magnitude of the other elements(absolute value) in the same row.
\chapter{Unidirectional/Line search}
\label{sec:org2b9409a}
For a given function,

Find the minima of \(f(x) = (x-1)^2 + (y-2)^2\) starting from \((0,0)\) along the direction of x-axis.

How to perform uni-directional search:
\begin{itemize}
\item Write the parametric representation of the line search, $$\vec{s}(t) = \vec{a} + t\vec{b}$$
\item Write the function in terms of \(t\), \(f(\vec{s}(t))\). this is a single variable function.
\item Find the minimum of \(f(\vec{s}(t))\), \(t^*\)
\item Obtain the solution for unidirectional search by substituting \(t^*\) in \(\vec{s}(t)\)

Find the minimum of the function \(f(x) = (x-2)^2 - y\), starting from the point \((-1,0)\) along \((1,1)^T\)
\end{itemize}
\chapter{Euler's Method}
\label{sec:org43d415d}
\part{Overview}
\label{sec:org7a4b9cb}
\chapter{Calculating Eigenvalues and Eigenvectors}
\label{sec:org105709d}
\chapter{Norms Of Vectors}
\label{sec:orgd0f0880}
\section{\(L_n\) norm}
\label{sec:org4c3fb4d}
The \(L_n\) norm of a vector \(x_{m \times 1}\) is,

$$L_n =\sqrt[n]{x_1^n + x_2^n + \cdots + x_m^n}$$

\(L_\infty = max{|x_1|, |x_2|, \cdots , |x_n|}\)
\section{S-norm}
\label{sec:org99de82f}
Where S is a symmetric positive matrix

\(\|v\|_s = v^T S v\)
\chapter{Norms Of Matrices}
\label{sec:orgc10a471}
\section{Spectral Norm}
\label{sec:orgb67607e}
It is the maximum of the ratio,

$$\frac{\|Ax\|}{\|x\|}$$

The maximum is \(\sigma_1\) at the vector \(x=v_1\)
\section{Frobenius Norm}
\label{sec:org5145e70}

$$\|A\|_F = \sqrt{\Sigma_i \Sigma_j a_{ij}^2}$$

It's the same as the vector norm of the vector of singular values of \(A\)
\section{Nuclear Norm}
\label{sec:org9b60fb6}

$$\|A\|_N = \sigma_1 + \sigma_2 + \cdots + \sigma_r$$
\chapter{Optimization}
\label{sec:org63a9021}
\section{Types Of Optimization Problems}
\label{sec:org34433ea}
Single variable and multi variable
\begin{enumerate}
\item Constrained and unconstrained
\item Linear and nonlinear
\item Continuous and discrete
\item oadsfijdsoifjdsoifj
\item Single-objective and multi-objective
\item Stochastic and deterministic
\end{enumerate}
\end{document}
